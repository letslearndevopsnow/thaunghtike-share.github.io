---
layout:     post
title:   "Backup And Restore Kubernetes Resources Using Velero"
date:       2021-09-09 15:13:18 +0200
image: 25.jpg
tags:
    - Kubernetes
categories: kubernetes
---            

<h2> Introduction </h2>

We have to back up the applications which are deployed on a Kubernetes cluster to protect loss. There are many tools to backup and restore k8s resources like velero and Kasten 10. If you have backups, it is easy to restore if the cluster goes down or an application crashes accidentally. You can also back up k8s resources and persistent volumes on Kubernetes clusters. You can use velero not only to back up and restore but also to migrate clusters. For example, to migrate the PVC used for the database in the on-prem k8s cluster, you have to export database dumps from the old cluster at first then import the dumped databases to the database in the new cluster. Velero can solve these problems. When k8s clusters are migrated, there are limitations to migrating to PVC. The storage class of the PVC which you want to backup must be in the new cluster. You have to use same common bucket if you want to migrate resources among multiple clusters. Velero does not support host path PVC. 

<h2> Installation </h2>

Firstly you have to create a kubernetes cluster. I will use azure aks cluster with two worker nodes for this demo. So, let me create an aks cluster.

```bash
thaunghtikeoo@thaunghtikeoo:~$ az group create --name demo --location eastus && az aks create --name demo -g demo --location eastus --node-count 2 
```
My k8s cluster is ready now. Check the status of cluster's nodes.

```bash
thaunghtikeoo@thaunghtikeoo:~$ kubectl get nodes 
NAME                                STATUS   ROLES   AGE    VERSION
aks-nodepool1-32685854-vmss000000   Ready    agent   112s   v1.20.9
aks-nodepool1-32685854-vmss000001   Ready    agent   2m1s   v1.20.9
```
Then you have to install velero cli binary on your machine. You can download velero cli [here](https://github.com/vmware-tanzu/velero/releases/tag/v1.6.3). After downloading binary, extract the tarball and move velero to /usr/local/bin.

```bash
thaunghtikeoo@thaunghtikeoo:~$ wget https://github.com/vmware-tanzu/velero/releases/download/v1.6.3/velero-v1.6.3-linux-amd64.tar.gz

thaunghtikeoo@thaunghtikeoo:~$ tar xzvf velero-v1.6.3-linux-amd64.tar.gz && chmod +x velero-v1.6.* && sudo mv velero-v1.6.3-linux-amd64/velero /usr/local/bin/velero
```
<h2> Velero supported providers </h2>

Velero supports a variety of storage providers for different backup and snapshot operations. You can check [here](https://velero.io/docs/v1.6/supported-providers) to see list of supported providers. I will use Amazon S3 bucket today. You can use Azure Blog Storage or Google Cloud Storage as you want. Before installing Velero on kubernetes cluster, create a S3 bucket which will be used to store backup and restore tar files.

```bash
BUCKET=tho-velero
REGION=us-east-1
aws s3api create-bucket \
    --bucket $BUCKET \
    --region $REGION 
```
Now, I'm ready to install VELERO on my aks cluster. 

```bash
thaunghtikeoo@thaunghtikeoo:~$ velero install --use-restic --provider aws --plugins velero/velero-plugin-for-aws:v1.2.1 --bucket $BUCKET --backup-location-config region=$REGION --snapshot-location-config region=$REGION --secret-file ~/.aws/credentials 
```
You will get like the outputs below after a couple of minutes.

```bash
Deployment/velero: attempting to create resource
Deployment/velero: attempting to create resource client
Deployment/velero: created
Velero is installed! ⛵ Use 'kubectl logs deployment/velero -n velero' to view the status.
```
You will see restic daemonset here. So, what's restic? Let me explain a little bit about restic.

<h2> Restic </h2>

Velero supports backing up and restoring Kubernetes volumes using a free open-source backup tool called restic. Velero allows you to take snapshots of persistent volumes as part of your backups if you’re using one of the supported cloud providers’ block storage offerings (Amazon EBS Volumes, Azure Managed Disks, Google Persistent Disks). It means if you are using one of the cloud storage providers such as EBS or Azure Disks , it's not a problem why you can create snapshots manually. Then you can create pvc with new volumes which are created from snapshots. However, if you want to use storage platform that doesn’t have a native snapshot concept such as ( Azure File, EFS ), restic might be for you. If you don't use restic, velero only restores pvc name without restoring data inside volume.

<h3> Install restic </h3>

To install restic, use the --use-restic flag in the velero install command. See the install overview for more details on other flags for the install command.

```bash
velero install --use-restic
```
<h2> Deploy Sample App </h2>

Let me create a simple busybox which will run for one hour. Azure Disk Storage Class is the default storage for pvc volumes on AKS.

```bash
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvcdemo
spec:
  storageClassName: default
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi

---

apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  volumes:
  - name: demovol
    persistentVolumeClaim:
      claimName: pvcdemo
  containers:
  - image: busybox
    name: busybox
    command: ["/bin/sh"]
    args: ["-c", "sleep 3600"]
    volumeMounts:
    - name: demovol
      mountPath: /mydata
```
Create busybox app with 'kubectl create' command. You will see busybox pod is running after some minutes. 

```bash
thaunghtikeoo@thaunghtikeoo:~$ kubectl get pods
NAME      READY   STATUS    RESTARTS   AGE
busybox   1/1     Running   0          103s
```
Also you got one pvc.

```bash
thaunghtikeoo@thaunghtikeoo:~$ kubectl get pvc
NAME       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvcdemo   Bound    pvc-b7d55de2-ec55-409b-9d8c-402d7cc01b55      2Gi        RWO            default     2m47s
```
Create some files inside busybox container's mount path to test persistent data.

```bash
$ kubectl exec busybox -- touch /mydata/a /mydata/b /mydata/c
$ kubectl exec busybox -- ls /mydata
a
b
c
```
It's time to test backup and restore kubernetes resources. Don't forget to add velero backup annotation to your pod which you want to back up. If you don't annotate, velero won't restore data inside volumes to newly created pvc.

Run the following for each pod that contains a volume to back up: If you want to exclude some volumes to back up , use this annotation - 'backup.velero.io/backup-volumes-excludes=volume1,volume2'

```bash
$ kubectl annotate pod/busybox backup.velero.io/backup-volumes=demovol
```
<h2> Testing Back Up And Restore </h2>

Firstly, create a backup for busybox pod and pvc. You can also create schedule backups. 

```bash
$ velero create backup busybox-bk --include-namespaces default --include-resources pod,pvc,pv
```
Now we have a velero backup for busybox pod and pvc. Default ttl for a backup is 3 month. You have to create a new backup after expire.

```
thaunghtikeoo@thaunghtikeoo:~$ velero get backups
NAME         STATUS      ERRORS   WARNINGS   CREATED                           EXPIRES   STORAGE LOCATION   SELECTOR
busybox-bk   Completed   0        0          2021-09-09 15:24:57 +0630 +0630   29d       default            <none>
```
Go to S3 bucket and you will see backup folder named busybox-bk. Velero stores backup resources inside this repo.

![velero_backup](https://raw.githubusercontent.com/thaunghtike-share/thaunghtike-share.github.io/master/images/velero_backup.png)

You may also know which resources are backed up by velero with 'velero describe backup name --details' command. Also you can check backup logs with 'velero backup logs name' command.

```bash
$ velero describe backup busybox-bk --details
Resource List:
  v1/PersistentVolume:
    - pvc-b7d55de2-ec55-409b-9d8c-402d7cc01b55
  v1/PersistentVolumeClaim:
    - default/pvc-demo
  v1/Pod:
    - default/busybox
```
Yeah you're ready to try restoring k8s resources with velero. Delete busybox pod and pvc.

```bash
$ kubectl delete pod/busybox --force && kubectl delete pv,pvc --all --force
```
Use 'velero create backup' to restore kubernetes resources and pvc.

```bash
$ velero create restore busybox-rs --from-backup busybox-bk
```
Wait for some minutes to complete restoring backup.

```bash
thaunghtikeoo@thaunghtikeoo:~$ velero get restores
NAME      BACKUP       STATUS      STARTED                           COMPLETED                         ERRORS   WARNINGS   CREATED                           SELECTOR
busybox   busybox-bk   Completed   2021-09-09 19:17:34 +0630 +0630   2021-09-09 19:17:59 +0630 +0630   0        1          2021-09-09 19:17:34 +0630 +0630   <none>
```
You will see busybox pod running after restoring.

```
$ thaunghtikeoo@thaunghtikeoo:~$ kubectl get pods
NAME          READY   STATUS    RESTARTS   AGE
pod/busybox   1/1     Running   0          44s
```
Check busybox container's mount path again.

```bash
$ thaunghtikeoo@thaunghtikeoo:~$ kubectl exec busybox -- ls /mydata
Defaulted container "busybox" out of: busybox, restic-wait (init)
a
b
c
lost+found
```
So, restic works well. You got some files inside container's mount path. Describe busybox pod.

```bash
$ kubectl describe pod/busybox
Init Containers:
  restic-wait:
    Container ID:  containerd://6aad3810d8c4b69979886c665994d2812ea5ba950d618612ed4c8b76c6dfe489
    Image:         velero/velero-restic-restore-helper:v1.6.3
    Image ID:      docker.io/velero/velero-restic-restore-helper@sha256:e4a7ea4e66faf79eb920a81aa06745957332c2ea61779ec4686b4b9518f5aa33
    Port:          <none>
    Host Port:     <none>
    Command:
      /velero-restic-restore-helper
```
Velero use restic container as init container to restore persistent data to the container. Go back to S3 bucket again. You will see busybox restore folder. 

![velero_restore](https://raw.githubusercontent.com/thaunghtike-share/thaunghtike-share.github.io/master/images/velero_restore.png)

<h2> Conclusion </h2>

Install Velero in another cluster pointing to the same S3 bucket to restore resources to another cluster. Hope velero can help you and your works a lot. Try velero because it's an open-source project. Thanks for reading ....
